{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 2087 × 100\n",
       "    obs: 'clusters_coarse', 'clusters', 'S_score', 'G2M_score', 'cell_type', 'sizeFactor', 'pseudotime'\n",
       "    var: 'highly_variable_genes'\n",
       "    uns: 'X_name', 'clusters_coarse_colors', 'clusters_colors', 'day_colors', 'neighbors', 'pca'\n",
       "    obsm: 'PCA', 'UMAP', 'X_pca', 'X_umap'\n",
       "    layers: 'counts', 'cpm', 'logcounts', 'spliced', 'unspliced'\n",
       "    obsp: 'connectivities', 'distances'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import anndata\n",
    "import os\n",
    "import requests\n",
    "\n",
    "save_path = \"data/example_sce.h5ad\"\n",
    "if not os.path.exists(save_path):\n",
    "    response = requests.get(\"https://go.wisc.edu/69435h\")\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "example_sce = anndata.read_h5ad(save_path)\n",
    "example_sce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memento model works directly off the matrix of transcript counts. Therefore, we don't need to keep track of the cell-level metadata that are used as predictors in other models. This implementation loads all data at once, though the fact that it works off sparse matrices means that it still is quite memory efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scdesigner.experimental.estimators import MementoEstimator\n",
    "from scdesigner.experimental.data import SparseMatrixLoader\n",
    "\n",
    "memento = MementoEstimator(q=0.01)\n",
    "sml = SparseMatrixLoader(example_sce, batch_size=1000)\n",
    "fit = memento.estimate(sml.loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9258, 0.0935, 0.0788,  ..., 0.0000, 0.0339, 0.0000],\n",
       "        [0.0935, 1.2218, 0.1555,  ..., 0.0000, 0.0601, 0.0000],\n",
       "        [0.0788, 0.1555, 1.7056,  ..., 0.0000, 0.0570, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 2.6681, 0.0000, 0.0000],\n",
       "        [0.0339, 0.0601, 0.0570,  ..., 0.0000, 0.2188, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.9393]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit[\"covariance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These means seem too small, even with a very small sampling fraction $q$. Did we implement the formulas incorrectly, or is the paper incorrect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9964, 0.6870, 0.9047, 0.9294, 0.7371, 0.8868, 1.0727, 0.4265, 0.6228,\n",
       "        0.3072, 1.1863, 1.5618, 0.9423, 1.4388, 1.5394, 0.8234, 1.0267, 0.7484,\n",
       "        1.1656, 1.0978, 0.7334, 0.6444, 0.9384, 0.9762, 1.4089, 0.6524, 0.8391,\n",
       "        1.2052, 1.2606, 0.6162, 1.1437, 0.7583, 0.8251, 1.3618, 0.7300, 1.1556,\n",
       "        1.2105, 1.4168, 0.8674, 1.6186, 0.6873, 1.2246, 1.0389, 0.9409, 1.3263,\n",
       "        0.7447, 1.3467, 1.3459, 0.3118, 0.6714, 0.5819, 1.3055, 1.5540, 1.0572,\n",
       "        0.6961, 0.8785, 1.0091, 1.3071, 0.6647, 0.9972, 1.1098, 1.6534, 0.6749,\n",
       "        0.7481, 0.8377, 1.4986, 1.3135, 1.1631, 1.3434, 1.1374, 0.9192, 0.5181,\n",
       "        0.8057, 0.5359, 1.0155, 0.4667, 0.1773, 0.7219, 1.0304, 1.2584, 1.1573,\n",
       "        0.9930, 0.8115, 0.9604, 1.4384, 0.7189, 1.6503, 0.6454, 0.9660, 1.5492,\n",
       "        1.4248, 1.1118, 1.4927, 1.1321, 1.2761, 1.1052, 1.3150, 1.1487, 0.0911,\n",
       "        0.9299])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit[\"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mysc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
